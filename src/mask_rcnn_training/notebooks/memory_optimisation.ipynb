{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dd373c-928e-4d27-9a7d-3e2d8392fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "loading annotations into memory...\n",
      "Done (t=12.81s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=3.92s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from training_utils.ring_mask_converter import process\n",
    "from training_utils.utils import eval_map\n",
    "from training_utils.model_builder import get_model\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from training_utils.utils import get_nvidia_gpu_memory\n",
    "\n",
    "\n",
    "with open('training_configs.json', 'r') as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 \n",
    "\n",
    "device = torch.device('cuda:0' if USE_GPU and torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "transforms = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "train_images_path = configs['train_images_path']\n",
    "train_annotations_path = train_images_path + '/result.json'\n",
    "\n",
    "val_images_path = configs['val_images_path']\n",
    "val_annotations_path = val_images_path + '/result.json'\n",
    "\n",
    "train_dataset = datasets.CocoDetection(train_images_path, train_annotations_path, transforms=transforms)\n",
    "train_dataset = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
    "\n",
    "val_dataset = datasets.CocoDetection(val_images_path, val_annotations_path, transforms=transforms)\n",
    "val_dataset = datasets.wrap_dataset_for_transforms_v2(val_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
    "\n",
    "model_path = configs['model_path'] \n",
    "num_classes = len(['Seed','Interior','Endosperm','Void']) + 1 # add a class for background\n",
    "model = get_model(num_classes, model_path=model_path).to(device) if configs['load_from_checkpoint'] else get_model(num_classes, model_path=None).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=configs['train_batch_size'], collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=configs['val_batch_size'], collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "\n",
    "print_every = configs['print_every']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aae7b25-048a-4256-b2e7-159ad8799829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "model.train()\n",
    "\n",
    "images, targets = process(images, targets, device)\n",
    "loss_dict = model(images, targets)\n",
    "\n",
    "loss = sum(tuple([loss for loss in loss_dict.values()]))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\"\"\"\n",
    "optimizer.step()\n",
    "\"\"\"\n",
    "losses.append(loss.item())\n",
    "\n",
    "\n",
    "images_val, targets_val = next(iter(val_loader))\n",
    "images_val, target_val = process(images_val, targets_val, device)\n",
    "model.eval()\n",
    "preds_val = model(images_val)\n",
    "val_mAP = eval_map(preds_val, targets_val, device)\n",
    "\n",
    "preds_train = model(images)\n",
    "train_mAP = eval_map(preds_train, targets, device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e984c3e-3757-4148-9c6a-e1fc2e5c23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 14:11:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:01:00.0 Off |                   On |\n",
      "| N/A   33C    P0              30W / 165W |   1874MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |            1848MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               2MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0    1    0      18099      C   ...623/virtual_envs/nlp_env/bin/python     1816MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd318d3f-9425-4f4f-80b1-e2ebf1bc3623",
   "metadata": {},
   "source": [
    " - STEP - TOTAL USAGE\n",
    " - Model Only - 316MiB\n",
    " - Process images and targets - 316MiB\n",
    " - Model Forward Pass - 1652MiB\n",
    " - Sum Losses - 1652MiB\n",
    " - Zero Optim - 1652MiB\n",
    " - Backward Pass - 1816MiB\n",
    " - Optim step - 1856MiB\n",
    " - Garbage Collect - 1856MiB\n",
    " - Loading val data - 1856MiB\n",
    " - Process val data - 1856MiB\n",
    " - Model eval mode - 1856MiB\n",
    " - Val Forward Pass - 1958MiB\n",
    " - eval mAP - 1958MiB\n",
    " - Train forward pass - 2650MiB\n",
    " - Garbage Collection - 2650MiB\n",
    " - Empty Cache - 2412MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d603e00-a630-4bb5-a0fb-065086223c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters without LoRA: 43938541\n",
      "Trainable Parameters without LoRA: 43716141\n",
      "Total Parameters with LoRA: 44262946\n",
      "Trainable Parameters with LoRA: 324405\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_conv_layer_names(model):\n",
    "    conv_layer_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)):\n",
    "            conv_layer_names.append(name)\n",
    "    return conv_layer_names\n",
    "\n",
    "# Suppose 'model' is your pre-defined PyTorch model\n",
    "conv_layers = get_conv_layer_names(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters without LoRA: {total_params}\")\n",
    "print(f\"Trainable Parameters without LoRA: {trainable_params}\")\n",
    "\n",
    "# Define LoRA Configuration with convolutional layers as target modules\n",
    "lora_config = LoraConfig(\n",
    "    r=3,  # Rank of the adaptation\n",
    "    lora_alpha=3,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Optional: Dropout rate for LoRA weights\n",
    "    task_type=\"IMAGE_SEGMENTATION\",  # Adjust according to your task\n",
    "    target_modules=conv_layers  # Specify convolutional layers as target modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to your model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters with LoRA: {total_params}\")\n",
    "print(f\"Trainable Parameters with LoRA: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7079f-f988-4104-9310-f44f9aebf494",
   "metadata": {},
   "source": [
    " - STEP - TOTAL USAGE\n",
    " - Model Only - 320MiB\n",
    " - Process images and targets - 320MiB\n",
    " - Model Forward Pass - 2992MiB\n",
    " - Backward Pass - 3016MiB\n",
    " - Optim step - 3016MiB\n",
    " - Forward pass of val - 3104MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6ba8c54-3be8-4839-bf2d-8005b936fd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader. Mem: 528\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m start_mem \u001b[38;5;241m=\u001b[39m get_nvidia_gpu_memory()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     81\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mrun_one_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     84\u001b[0m end_mem \u001b[38;5;241m=\u001b[39m get_nvidia_gpu_memory()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m, in \u001b[0;36mrun_one_pass\u001b[0;34m(model, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Register the hook onto every parameter\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_post_accumulate_grad_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Now remember our previous `⁠ train() ⁠` function? Since the optimizer has been\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# fused into the backward, we can remove the optimizer step and zero_grad calls.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# create our fake image input: tensor shape is batch_size, channels, height, width\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/or623/virtual_envs/nlp_env/lib/python3.10/site-packages/torch/_tensor.py:616\u001b[0m, in \u001b[0;36mTensor.register_post_accumulate_grad_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    613\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mregister_post_accumulate_grad_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot register a hook on a tensor that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost accumulate grad hooks cannot be registered on non-leaf tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    622\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    }
   ],
   "source": [
    "from training_utils.utils import get_nvidia_gpu_memory\n",
    "\n",
    "def run_one_pass(model, batch_size):\n",
    "    printing = True\n",
    "    training = True\n",
    "    if printing:\n",
    "        print_fn = lambda x: print(f\"{x} {get_nvidia_gpu_memory()[0]['used']}\")\n",
    "    else:\n",
    "        print_fn = lambda x: x\n",
    "    if training:\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "        print_fn(\"Loader. Mem:\")\n",
    "\n",
    "        # Instead of having just one optimizer, we will have a `⁠ dict ⁠` of optimizers\n",
    "        # for every parameter so we could reference them in our hook.\n",
    "        optimizer_dict = {p: torch.optim.Adam([p], foreach=False) for p in model.parameters()}\n",
    "        model.train()\n",
    "        # Define our hook, which will call the optimizer `⁠ step() ⁠` and `⁠ zero_grad() ⁠`\n",
    "        def optimizer_hook(parameter) -> None:\n",
    "            optimizer_dict[parameter].step()\n",
    "            optimizer_dict[parameter].zero_grad()\n",
    "        \n",
    "        # Register the hook onto every parameter\n",
    "        for p in model.parameters():\n",
    "            p.register_post_accumulate_grad_hook(optimizer_hook)\n",
    "\n",
    "        # Now remember our previous `⁠ train() ⁠` function? Since the optimizer has been\n",
    "        # fused into the backward, we can remove the optimizer step and zero_grad calls.\n",
    "        def train(model):\n",
    "            # create our fake image input: tensor shape is batch_size, channels, height, width\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images, targets = process(images, targets, device)\n",
    "            # call our forward and backward\n",
    "            loss_dict = model.forward(fake_image)\n",
    "            #loss.sum().backward()\n",
    "            \n",
    "            loss = sum(tuple([loss for loss in loss_dict.values()]))\n",
    "            loss.backward()\n",
    "            # optimizer update --> no longer needed!\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "    \n",
    "        #optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
    "        #print_fn(\"Optimizer. Mem:\")\n",
    "        \n",
    "        model.train()\n",
    "        #print_fn(\"Load Data. Mem:\")\n",
    "        #images, targets = process(images, targets, device)\n",
    "        #print_fn(\"Process Data. Mem:\")\n",
    "        #loss_dict = model(images, targets)\n",
    "        #print_fn(\"Forward Pass. Mem:\")\n",
    "        #loss = sum(tuple([loss for loss in loss_dict.values()]))\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #print_fn(\"Backward Pass. Mem:\")\n",
    "        #optimizer.step()\n",
    "        #print_fn(\"Step. Mem:\")\n",
    "        train(model)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "            print_fn(\"Loader. Mem:\")\n",
    "            images, targets = next(iter(train_loader))\n",
    "            model.eval()\n",
    "            print_fn(\"Load Data. Mem:\")\n",
    "            images, targets = process(images, targets, device)\n",
    "            print_fn(\"Process Data. Mem:\")\n",
    "            model(images)\n",
    "            print_fn(\"Forward Pass. Mem:\")\n",
    "\n",
    "\n",
    "        \n",
    "mems = []\n",
    "sizes = []\n",
    "import gc\n",
    "import time\n",
    "for batch_size in range(1, 5):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    start_mem = get_nvidia_gpu_memory()[0]['used']\n",
    "    start_time = time.time()\n",
    "    run_one_pass(model, batch_size)\n",
    "    end_time = time.time()\n",
    "    end_mem = get_nvidia_gpu_memory()[0]['used']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Batch Size: {batch_size} | Start Memory: {start_mem} | End Memory: {end_mem} | Time: {end_time - start_time}\")\n",
    "    sizes.append(batch_size)\n",
    "    mems.append((end_mem - start_mem)/1024)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sizes, mems, 'x',label='No Lora')\n",
    "plt.xlabel('Batch Size - Images')\n",
    "plt.ylabel('Memory - GiB')\n",
    "\n",
    "\n",
    "mems = []\n",
    "sizes = []\n",
    "for batch_size in range(1, 5):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    start_mem = get_nvidia_gpu_memory()[0]['used']\n",
    "    start_time = time.time()\n",
    "    run_one_pass(lora_model, batch_size)\n",
    "    end_time = time.time()\n",
    "    end_mem = get_nvidia_gpu_memory()[0]['used']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Batch Size: {batch_size} | Start Memory: {start_mem} | End Memory: {end_mem} | Time: {end_time - start_time}\")\n",
    "    sizes.append(batch_size)\n",
    "    mems.append((end_mem - start_mem)/1024)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sizes, mems, 'o', label='Lora')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a9f682-a7ce-4aaf-858f-1e33c93c1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory required: 40520 bytes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_memory(obj):\n",
    "    \"\"\"\n",
    "    Recursively calculates the total memory required to store all tensors\n",
    "    in a nested structure consisting of lists, dicts, or tuples.\n",
    "\n",
    "    Args:\n",
    "    - obj: The nested structure containing tensors.\n",
    "\n",
    "    Returns:\n",
    "    - The total memory required to store the tensors, in bytes.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.nelement() * obj.element_size()\n",
    "    elif isinstance(obj, dict):\n",
    "        return sum(calculate_memory(v) for v in obj.values())\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return sum(calculate_memory(item) for item in obj)\n",
    "    else:\n",
    "        return 0  # Non-tensor objects are not counted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa20066-0736-431c-a816-697b5601435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "model.train()\n",
    "\n",
    "images, targets = process(images, targets, device)\n",
    "loss_dict = model(images, targets)\n",
    "\n",
    "loss = sum(tuple([loss for loss in loss_dict.values()]))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\"\"\"\n",
    "optimizer.step()\n",
    "\"\"\"\n",
    "losses.append(loss.item())\n",
    "\n",
    "\n",
    "images_val, targets_val = next(iter(val_loader))\n",
    "images_val, target_val = process(images_val, targets_val, device)\n",
    "model.eval()\n",
    "preds_val = model(images_val)\n",
    "val_mAP = eval_map(preds_val, targets_val, device)\n",
    "\n",
    "preds_train = model(images)\n",
    "train_mAP = eval_map(preds_train, targets, device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

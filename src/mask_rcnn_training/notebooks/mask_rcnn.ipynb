{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63836232-22f6-4f89-a619-d754ee6923e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "loading annotations into memory...\n",
      "Done (t=15.26s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "\n",
    "device = torch.device('cuda:0' if USE_GPU and torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "\n",
    "images_path = '/vol/bitbucket/ajm223/SWE_GP/data/sprint_5_aug_dataset_train'\n",
    "annotations_path = images_path + '/result.json'\n",
    "\n",
    "train_dataset = datasets.CocoDetection(images_path, annotations_path, transforms=transforms)\n",
    "train_dataset = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
    "\n",
    "images_path = '/vol/bitbucket/ajm223/SWE_GP/data/sprint_5_aug_dataset_val'\n",
    "annotations_path = images_path + '/result.json'\n",
    "\n",
    "val_dataset = datasets.CocoDetection(images_path, annotations_path, transforms=transforms)\n",
    "val_dataset = datasets.wrap_dataset_for_transforms_v2(val_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
    "\n",
    "\n",
    "batch_size = 20 # Adjust if memory is an issue\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, collate_fn=lambda batch: tuple(zip(*batch)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6ab59b-c4dd-4ba1-aaee-3d79c89b3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the priority order of the objects to only show the parts of the object that count as that region\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_anti_mask(image: torch.tensor, masks: torch.tensor, labels: torch.tensor, label_order: torch.tensor) -> torch.tensor:\n",
    "    total_masks = torch.zeros(size=(len(label_order), image.shape[1], image.shape[2]))\n",
    "    for i in range(len(labels)):\n",
    "        j = labels[i]\n",
    "        total_masks[j,:,:] = total_masks[j,:,:].bool() | masks[i,:,:].bool()\n",
    "        \n",
    "    total_masks = total_masks\n",
    "    out_masks = torch.zeros_like(total_masks)\n",
    "    #fig, ax = plt.subplots(1, len(label_order), sharex=True, sharey=True, dpi=200)\n",
    "    for i in range(len(label_order)):\n",
    "        order_ind = label_order.tolist().index(i)\n",
    "        #print(i, label_order[order_ind+1:])\n",
    "        out_masks[i,:,:] = torch.any(total_masks[label_order[order_ind+1:],:,:].view(-1, image.shape[1], image.shape[2]).bool(),dim=0)\n",
    "        #ax[i].imshow(out_masks[i,:,:])\n",
    "        #ax[i].axis('off')\n",
    "    #plt.show()\n",
    "    return out_masks\n",
    "\n",
    "def remove_overlap(image: torch.tensor, masks: torch.tensor, labels: torch.tensor, label_order, display=False) -> torch.tensor:\n",
    "    anti_masks = get_anti_mask(image, masks, labels, label_order)\n",
    "    masks_out = torch.zeros_like(masks)\n",
    "    for i in range(len(masks)):\n",
    "        masks_out[i,:,:] = masks[i,:,:].bool() & ~anti_masks[labels[i],:,:].bool()\n",
    "        if display:\n",
    "            print([\"Seed Coat\", \"Interior\", \"Endosperm\", \"Void\"][labels[i]])\n",
    "            fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, dpi=200)\n",
    "            \n",
    "            ax[0].imshow(masks[i,:,:])\n",
    "            ax[0].set_title(\"Mask\")\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(anti_masks[labels[i],:,:])\n",
    "            ax[1].set_title(\"Anti-Mask\")\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            ax[2].imshow(masks_out[i,:,:])\n",
    "            ax[2].set_title(\"Output\")\n",
    "            ax[2].axis('off')\n",
    "\n",
    "            ax[3].imshow(image[2,:,:])\n",
    "            ax[3].set_title(\"Image\")\n",
    "            ax[3].axis('off')\n",
    "            \n",
    "            plt.show()\n",
    "    return masks_out\n",
    "\n",
    "def process(images, targets, display=False):\n",
    "    out_images, out_targets = [], []\n",
    "    label_order = torch.tensor([0, 1, 2, 3])\n",
    "    for i in range(len(images)):\n",
    "        out_images.append(images[i].to(device))\n",
    "        out_targets.append({\n",
    "            'boxes':targets[i]['boxes'].to(device),\n",
    "            'masks':remove_overlap(images[i], targets[i]['masks'], targets[i]['labels'], label_order, display=display).to(device),\n",
    "            'labels':targets[i]['labels'].to(device)\n",
    "        })\n",
    "    return tuple(out_images), tuple(out_targets)\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "def show_progress(image, target, num_masks=1):\n",
    "    fig, ax = plt.subplots(2,1+num_masks,sharex=True, sharey=True, dpi=300, figsize=(3*(1+num_masks),6))\n",
    "\n",
    "    cmap_im = None\n",
    "    cmap_mask = 'jet'\n",
    "\n",
    "    # display original image\n",
    "    im = image.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    im = (im - im.min()) / (im.max() - im.min())\n",
    "\n",
    "    ax[0,0].imshow(im,cmap=cmap_im)\n",
    "    #ax[0,0].imshow(im*0,cmap=cmap_mask, alpha=0.5)\n",
    "    ax[1,0].set_title('Original')\n",
    "    \n",
    "    ax[1,0].imshow(im,cmap=cmap_im)\n",
    "    #ax[1,0].imshow(im*0,cmap=cmap_mask, alpha=0.5)\n",
    "    ax[0,0].set_title('BBoxes')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(image.unsqueeze(0).to(device))[0]\n",
    "        model.train()\n",
    "        \n",
    "    for j in range(num_masks):\n",
    "        i = random.randint(0, len(output['masks'])-1)\n",
    "        maps = output['masks'][i,:,:].detach().cpu().squeeze(0).numpy()\n",
    "        ax[0,j+1].imshow(im,cmap=cmap_im)\n",
    "        ax[0,j+1].imshow(maps,cmap=cmap_mask, alpha=0.5)\n",
    "        \n",
    "        label = output['labels'][i].item()\n",
    "        score = output['scores'][i].item()\n",
    "        class_ = [\"Seed Coat\", \"Interior\", \"Endosperm\", \"Void\"][label]\n",
    "        \n",
    "        ax[0,j+1].set_title(f\"Pred: {class_} - {score*100:3.1f}%\")\n",
    "\n",
    "    for j in range(num_masks):\n",
    "        i = random.randint(0, len(target['masks'][:,0,0])-1)\n",
    "        maps = target['masks'][i,:,:].detach().cpu().squeeze(0).numpy()\n",
    "        ax[1,j+1].imshow(im,cmap=cmap_im)\n",
    "        ax[1,j+1].imshow(maps,cmap=cmap_mask, alpha=0.5)\n",
    "        \n",
    "        label = target['labels'][i].item()\n",
    "        #score = target['l'][i].item()\n",
    "        class_ = [\"Seed Coat\", \"Interior\", \"Endosperm\", \"Void\"][label]\n",
    "        \n",
    "        ax[1,j+1].set_title(f\"Target: {class_}\")\n",
    "\n",
    "    for j in range(len(output['boxes'])):\n",
    "        x1, y1, x2, y2 = output['boxes'][j].detach().cpu().numpy()\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax[0,0].add_patch(rect) \n",
    "\n",
    "    for j in range(len(target['boxes'])):\n",
    "        x1, y1, x2, y2 = target['boxes'][j].detach().cpu().numpy()\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax[1,0].add_patch(rect) \n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(1+num_masks):\n",
    "            ax[i,j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision as mAP\n",
    "def eval_map(preds, targets):\n",
    "    metric = mAP().to(device)\n",
    "    targets = NestedTensorHandler.get_structure_on_device(list(targets), device)\n",
    "    preds = NestedTensorHandler.get_structure_on_device(preds, device)\n",
    "    output = metric(preds, targets)\n",
    "    return output['map']\n",
    "\n",
    "class NestedTensorHandler:\n",
    "    @staticmethod\n",
    "    def to_device(item, device):\n",
    "        \"\"\"Recursively send tensors to the specified device in the nested structure.\"\"\"\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            # Move tensor to the specified device\n",
    "            return item.to(device)\n",
    "        elif isinstance(item, dict):\n",
    "            # Recursively process dictionary items\n",
    "            return {k: NestedTensorHandler.to_device(v, device) for k, v in item.items()}\n",
    "        elif isinstance(item, list):\n",
    "            # Recursively process list items\n",
    "            return [NestedTensorHandler.to_device(i, device) for i in item]\n",
    "        elif isinstance(item, tuple):\n",
    "            # Recursively process tuple items and convert it back to tuple\n",
    "            return tuple(NestedTensorHandler.to_device(i, device) for i in item)\n",
    "        else:\n",
    "            # Return the item as is if it's not a tensor, list, dict, or tuple\n",
    "            return item\n",
    "\n",
    "    @staticmethod\n",
    "    def get_structure_on_device(nested_structure, device='cpu'):\n",
    "        \"\"\"Return the nested structure with tensors moved to the specified device.\"\"\"\n",
    "        return NestedTensorHandler.to_device(nested_structure, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1188da8d-90a4-4ba8-894e-e20bda2534a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model(num_classes, model_path=None):\n",
    "    # Load a pre-trained model for classification and return\n",
    "    # only the features\n",
    "    model = maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # And replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    if not model_path is None:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4c3768-f45d-4972-a1e9-27ab5fde385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = True\n",
    "\n",
    "model_path = 'model.pt'\n",
    "num_classes = len(['Seed','Interior','Endosperm','Void']) + 1\n",
    "model = get_model(num_classes).to(device)\n",
    "if load_from_checkpoint:\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350fd9ba-b9ab-4047-bf45-ea979f5f69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val, targets_val = next(iter(val_loader))\n",
    "image_val, target_val = process(images_val, targets_val)\n",
    "#show_progress(image_val[0], target_val[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3d2a1-794c-4651-8751-9e227ec8b681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a87ed51f353493ebd3f6490a2386d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5ff32aff7940feaac8dafed2619b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Iteration 0 | Loss 0.3520944118499756 | Train mAP: 0.5038595199584961 | Val mAP: 0.4518515169620514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f0c4cfe1cc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/bitbucket/or623/virtual_envs/nlp_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 1 # Set the number of epochs\n",
    "batch_size = 20 # Adjust if memory is an issue\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, collate_fn=lambda batch: tuple(zip(*batch)),)\n",
    "\n",
    "print_every = len(train_loader)//5\n",
    "\n",
    "model.to(device)\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "    for i, (images, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            model.train()\n",
    "            images, targets = process(images, targets)\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(tuple([loss for loss in loss_dict.values()]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            gc.collect()\n",
    "            if i % print_every == print_every-1 or i == 0:\n",
    "                images_val, targets_val = next(iter(val_loader))\n",
    "                images_val, target_val = process(images_val, targets_val)\n",
    "                model.eval()\n",
    "                preds_val = model(images_val)\n",
    "                val_mAP = eval_map(preds_val, targets_val)\n",
    "                \n",
    "                preds_train = model(images)\n",
    "                train_mAP = eval_map(preds_train, targets)\n",
    "                tqdm.write(f\"Epoch {epoch} | Iteration {i} | Loss {loss.item():1.3f} | Train mAP: {train_mAP.item():1.3f} | Val mAP: {val_mAP.item():1.3f}\")\n",
    "\n",
    "                #show_progress(image_val[0], target_val[0], 4)\n",
    "                \n",
    "                \n",
    "load_from_checkpoint = True\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092100a-6861-47a5-a672-a3c868289197",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val, targets_val = next(iter(val_loader))\n",
    "images_val, targets_val = process(images_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cd25b-20de-4015-bd9c-d11fca6940e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(images_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510bf3a-0263-42b3-865a-a891b6ee4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(image, target, num_masks=1):\n",
    "    fig, ax = plt.subplots(2,1+num_masks,sharex=True, sharey=True, dpi=300, figsize=(3*(1+num_masks),6))\n",
    "\n",
    "    cmap_im = None\n",
    "    cmap_mask = 'jet'\n",
    "\n",
    "    # display original image\n",
    "    im = image.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    im = (im - im.min()) / (im.max() - im.min())\n",
    "\n",
    "    ax[0,0].imshow(im,cmap=cmap_im)\n",
    "    #ax[0,0].imshow(im*0,cmap=cmap_mask, alpha=0.5)\n",
    "    ax[1,0].set_title('Original')\n",
    "    \n",
    "    ax[1,0].imshow(im,cmap=cmap_im)\n",
    "    #ax[1,0].imshow(im*0,cmap=cmap_mask, alpha=0.5)\n",
    "    ax[0,0].set_title('BBoxes')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(image.unsqueeze(0).to(device))[0]\n",
    "        model.train()\n",
    "        \n",
    "    for j in range(num_masks):\n",
    "        i = random.randint(0, len(output['masks'])-1)\n",
    "        maps = output['masks'][i,:,:].detach().cpu().squeeze(0).numpy()\n",
    "        ax[0,j+1].imshow(im,cmap=cmap_im)\n",
    "        ax[0,j+1].imshow(maps,cmap=cmap_mask, alpha=0.5)\n",
    "        \n",
    "        label = output['labels'][i].item()\n",
    "        score = output['scores'][i].item()\n",
    "        class_ = [\"Seed Coat\", \"Interior\", \"Endosperm\", \"Void\"][label]\n",
    "        \n",
    "        ax[0,j+1].set_title(f\"Pred: {class_} - {score*100:3.1f}%\")\n",
    "\n",
    "    for j in range(num_masks):\n",
    "        i = random.randint(0, len(target['masks'][:,0,0])-1)\n",
    "        maps = target['masks'][i,:,:].detach().cpu().squeeze(0).numpy()\n",
    "        ax[1,j+1].imshow(im,cmap=cmap_im)\n",
    "        ax[1,j+1].imshow(maps,cmap=cmap_mask, alpha=0.5)\n",
    "        \n",
    "        label = target['labels'][i].item()\n",
    "        #score = target['l'][i].item()\n",
    "        class_ = [\"Seed Coat\", \"Interior\", \"Endosperm\", \"Void\"][label]\n",
    "        \n",
    "        ax[1,j+1].set_title(f\"Target: {class_}\")\n",
    "\n",
    "    for j in range(len(output['boxes'])):\n",
    "        x1, y1, x2, y2 = output['boxes'][j].detach().cpu().numpy()\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax[0,0].add_patch(rect) \n",
    "\n",
    "    for j in range(len(target['boxes'])):\n",
    "        x1, y1, x2, y2 = target['boxes'][j].detach().cpu().numpy()\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax[1,0].add_patch(rect) \n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(1+num_masks):\n",
    "            ax[i,j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764afdc6-a8eb-485b-b1fe-8450453891be",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val, targets_val = next(iter(val_loader))\n",
    "image_val, target_val = process(images_val, targets_val)\n",
    "show_progress(image_val[0], target_val[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170789f7-0bd4-4494-b2e5-b27c5a2b1c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics.utils import TQDM\n",
    "from ultralytics.utils.files import increment_path\n",
    "from ultralytics.data.converter import merge_multi_segment\n",
    "import zipfile\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"/vol/bitbucket/ajm223/SWE_GP/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Synth COCO zip file into a YOLO suitable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco(\n",
    "    labels_dir=\"../coco/annotations/\",\n",
    "    save_dir=\"coco_converted/\",\n",
    "    json_file=None,\n",
    "    use_segments=False,\n",
    "    use_keypoints=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts COCO dataset annotations to a YOLO annotation format suitable for training YOLO models.\n",
    "\n",
    "    Args:\n",
    "        labels_dir (str, optional): Path to directory containing COCO dataset annotation files.\n",
    "        save_dir (str, optional): Path to directory to save results to.\n",
    "        use_segments (bool, optional): Whether to include segmentation masks in the output.\n",
    "        use_keypoints (bool, optional): Whether to include keypoint annotations in the output.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from ultralytics.data.converter import convert_coco\n",
    "\n",
    "        convert_coco('../datasets/coco/annotations/', use_segments=True, use_keypoints=False, cls91to80=True)\n",
    "        ```\n",
    "\n",
    "    Output:\n",
    "        Generates output files in the specified output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create dataset directory\n",
    "    if os.path.exists(save_dir):\n",
    "        shutil.rmtree(save_dir)  # Delete the directory and all its contents\n",
    "\n",
    "    os.makedirs(save_dir)  # Create the directory again\n",
    "    save_dir = Path(save_dir)  # Convert to Path object\n",
    "\n",
    "    for p in save_dir / \"labels\", save_dir / \"images\":\n",
    "        p.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "\n",
    "    # Import json\n",
    "    # for json_file in sorted(Path(labels_dir).resolve().glob(\"*.json\")):\n",
    "    fn = Path(save_dir) / \"labels\"  # folder name\n",
    "    fn.mkdir(parents=True, exist_ok=True)\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        \n",
    "    # Create image dict\n",
    "    images = {f'{x[\"id\"]:d}': x for x in data[\"images\"]}\n",
    "    # Create image-annotations dict\n",
    "    imgToAnns = defaultdict(list)\n",
    "    for ann in data[\"annotations\"]:\n",
    "        imgToAnns[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # Write labels file\n",
    "    for img_id, anns in TQDM(imgToAnns.items(), desc=f\"Annotations {json_file}\"):\n",
    "        img = images[f\"{img_id:d}\"]\n",
    "        h, w, f = img[\"height\"], img[\"width\"], img[\"file_name\"]\n",
    "\n",
    "        bboxes = []\n",
    "        segments = []\n",
    "        keypoints = []\n",
    "        for ann in anns:\n",
    "            if ann[\"iscrowd\"]:\n",
    "                continue\n",
    "            # The COCO box format is [top left x, top left y, width, height]\n",
    "            box = np.array(ann[\"bbox\"], dtype=np.float64)\n",
    "            box[:2] += box[2:] / 2  # xy top-left corner to center\n",
    "            box[[0, 2]] /= w  # normalize x\n",
    "            box[[1, 3]] /= h  # normalize y\n",
    "            if box[2] <= 0 or box[3] <= 0:  # if w <= 0 and h <= 0\n",
    "                continue\n",
    "\n",
    "            cls = ann[\"category_id\"]  # class\n",
    "            box = [cls] + box.tolist()\n",
    "            if box not in bboxes:\n",
    "                bboxes.append(box)\n",
    "                if use_segments and ann.get(\"segmentation\") is not None:\n",
    "                    if len(ann[\"segmentation\"]) == 0:\n",
    "                        segments.append([])\n",
    "                        continue\n",
    "                    elif len(ann[\"segmentation\"]) > 1:\n",
    "                        s = merge_multi_segment(ann[\"segmentation\"])\n",
    "                        s = (np.concatenate(s, axis=0) / np.array([w, h])).reshape(-1).tolist()\n",
    "                    else:\n",
    "                        s = [j for i in ann[\"segmentation\"] for j in i]  # all segments concatenated\n",
    "                        s = (np.array(s).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()\n",
    "                    s = [cls] + s\n",
    "                    if cls == None:\n",
    "                        print(f\"cls is none for: {img_id}\")\n",
    "                    segments.append(s)\n",
    "                if use_keypoints and ann.get(\"keypoints\") is not None:\n",
    "                    keypoints.append(\n",
    "                        box + (np.array(ann[\"keypoints\"]).reshape(-1, 3) / np.array([w, h, 1])).reshape(-1).tolist()\n",
    "                    )\n",
    "\n",
    "        # Write\n",
    "        file_path = fn / f.split(\"/\")[1]  # Constructs the full path\n",
    "        file_path_with_suffix = file_path.with_suffix(\".txt\")  # Ensures the file has a .txt extension\n",
    "        # Create the parent directories if they don't exist\n",
    "        file_path_with_suffix.parent.mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "\n",
    "        with open(file_path_with_suffix, \"a\") as file:\n",
    "            for i in range(len(bboxes)):\n",
    "                if use_keypoints:\n",
    "                    line = (*(keypoints[i]),)  # cls, box, keypoints\n",
    "                else:\n",
    "                    line = (\n",
    "                        *(segments[i] if use_segments and len(segments[i]) > 0 else bboxes[i]),\n",
    "                    )  # cls, box or segments\n",
    "                if None not in line:\n",
    "                    file.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_dir_name = \"yoloDS6\"\n",
    "\n",
    "# create the new directory\n",
    "new_ds_dir = data_path + new_ds_dir_name\n",
    "os.makedirs(new_ds_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Assuming zip_file is the path to your zip file and data_path is your target directory\n",
    "zip_name = \"synth\"\n",
    "zip_file = data_path + zip_name + \".zip\"\n",
    "unzip_container =  data_path + new_ds_dir_name + \"/\"+ zip_name + \"_dir\"\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "os.makedirs(unzip_container, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_container)\n",
    "\n",
    "\n",
    "# get all the json files in the unzipped directory and move them to the new one\n",
    "for file in os.listdir(unzip_container):\n",
    "    if file.endswith(\".json\"):\n",
    "        os.rename(unzip_container + \"/\" + file, new_ds_dir + \"/\" + file)\n",
    "\n",
    "# now we make da yaml\n",
    "data = {\n",
    "    \"path\": new_ds_dir_name,\n",
    "    \"train\": \"../\" + new_ds_dir_name + \"/train\",\n",
    "    \"val\": \"../\" + new_ds_dir_name + \"/val\",\n",
    "    \"test\": \"../\" + new_ds_dir_name + \"/test\",\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"{data_path}{new_ds_dir_name}/synth_train.json\", \"r\") as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# # Update the class names in the data structure\n",
    "data[\"names\"] = {cat[\"id\"]: cat[\"name\"] for cat in js[\"categories\"]}\n",
    "\n",
    "# move the unzipped files to the new directory\n",
    "yaml_file_path = f\"{new_ds_dir}/{new_ds_dir_name}.yaml\"\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.safe_dump(data, file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "\n",
    "# here we create the yolo labels for each set using the corresponding jsons, remember, the synth dataset is the train set\n",
    "convert_coco(labels_dir=new_ds_dir + \"synth\",save_dir=new_ds_dir + \"/train\", json_file= f\"{new_ds_dir}/synth_train.json\" , use_segments=True)\n",
    "convert_coco(labels_dir=new_ds_dir + \"val\",save_dir=new_ds_dir + \"/val\", json_file= f\"{new_ds_dir}/synth_val.json\" , use_segments=True)\n",
    "#convert_coco(labels_dir=new_ds_dir + \"test\",save_dir=new_ds_dir + \"/test\", json_file= f\"{new_ds_dir}/test.json\" , use_segments=True)\n",
    "\n",
    "# now we need to move the images from the unzip container to the correct folder so they correspond to the labels\n",
    "\n",
    "# for the train set, simply rename the entire synth images folder\n",
    "train_images_dir = new_ds_dir + \"/train/images\"\n",
    "if os.path.exists(train_images_dir) and os.listdir(train_images_dir):\n",
    "    shutil.rmtree(train_images_dir)\n",
    "os.rename(unzip_container + \"/synth_train_images\", train_images_dir)\n",
    "\n",
    "# do the same for the synth val set\n",
    "val_images_dir = new_ds_dir + \"/val/images\"\n",
    "if os.path.exists(val_images_dir) and os.listdir(val_images_dir):\n",
    "    shutil.rmtree(val_images_dir)\n",
    "os.rename(unzip_container + \"/synth_val_images\", val_images_dir)\n",
    "\n",
    "# we need to do the test set differently since the images arent isolated in their folder\n",
    "#with open(f\"{new_ds_dir}/test.json\", \"r\") as f:\n",
    "    #js = json.load(f)\n",
    "    #for img in js[\"images\"]:\n",
    "        #os.rename(unzip_container + \"/images/\" + img[\"file_name\"].split(\"/\")[1], new_ds_dir + \"/test/images/\" + img[\"file_name\"].split(\"/\")[1])\n",
    "\n",
    "\n",
    "# remove the unzipped container\n",
    "shutil.rmtree(unzip_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define parameter grid\n",
    "\"\"\"\n",
    "grid = {\n",
    "    'batch': [24],\n",
    "    'device': [0],\n",
    "    'project': [\"segment\"],\n",
    "    'seed': [42],\n",
    "    'rect': [False, True],\n",
    "    'cos_lr': [False, True],\n",
    "    'amp': [False, True],\n",
    "    'fraction': [0.8, 1.0],\n",
    "    'lr0': [0.01, 0.001, 0.0001],\n",
    "    'lrf': [0.01, 1, 0.1],\n",
    "    'momentum': [0.937, 0.949, 0.95],\n",
    "    'weight_decay': [0.0005, 0.01, 0.1],\n",
    "    'warmup_epochs': [0, 3],\n",
    "    'overlap_mask': [True],\n",
    "    'dropout': [0.0, 0.1, 0.2],\n",
    "    'val': [True],\n",
    "    'plots': [True],\n",
    "    'epochs': [300],\n",
    "    'patience': [10, 20, 30]\n",
    "}\n",
    "\"\"\"\n",
    "grid = {\n",
    "    'batch': [24],\n",
    "    'device': [0],\n",
    "    'project': [\"segment\"],\n",
    "    'seed': [42],\n",
    "    'rect': [False],\n",
    "    'cos_lr': [False],\n",
    "    'amp': [False],\n",
    "    'fraction': [1.0],\n",
    "    'lr0': [0.01],\n",
    "    'lrf': [0.1],\n",
    "    'momentum': [0.937],\n",
    "    'weight_decay': [0.0005],\n",
    "    'warmup_epochs': [3],\n",
    "    'overlap_mask': [True],\n",
    "    'dropout': [0.0],\n",
    "    'val': [True],\n",
    "    'plots': [True],\n",
    "    'epochs': [200],\n",
    "    'patience': [10]\n",
    "}\n",
    "\n",
    "ts = 6\n",
    "\n",
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "save_dir = \"/vol/bitbucket/ajm223/SWE_GP/runs\"\n",
    "epoch = 0\n",
    "\n",
    "for params in ParameterGrid(grid):\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    model = YOLO('yolov8n-seg.pt')\n",
    "    results = model.train(\n",
    "        data=f'/vol/bitbucket/ajm223/SWE_GP/data/yoloDS{ts}/yoloDS{ts}.yaml',\n",
    "        epochs=params['epochs'],\n",
    "        imgsz=1000,\n",
    "        device=params['device'],\n",
    "        batch=params['batch'],\n",
    "        rect=params['rect'],\n",
    "        cos_lr=params['cos_lr'],\n",
    "        amp=params['amp'],\n",
    "        fraction=params['fraction'],\n",
    "        lr0=params['lr0'],\n",
    "        lrf=params['lrf'],\n",
    "        momentum=params['momentum'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        warmup_epochs=params['warmup_epochs'],\n",
    "        overlap_mask=params['overlap_mask'],\n",
    "        dropout=params['dropout'],\n",
    "        val=params['val'],\n",
    "        plots=params['plots']\n",
    "    )\n",
    "    val_loss = results[0]['val_loss']\n",
    "    \n",
    "    # Save the model into the runs directory\n",
    "    model.save(os.path.join(save_dir, f'model_epoch_{epoch + 1}.pt'))\n",
    "    epoch += 1\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "        \n",
    "\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best model parameters: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_model.pt')\n",
    "\n",
    "# Save the best model parameters\n",
    "with open(\"best_model_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
